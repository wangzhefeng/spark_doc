.. _header-n0:


Spark ä»‹ç»
=========



Apache Spark
----------------

.. _header-n3:

1.Spark çš„è®¾è®¡å“²å­¦å’Œå†å²
~~~~~~~~~~~~~~~~~~~~~~~~~

Apache Spark is **a unified computing engine** and **a set of libraries
for parallel data processing(big data) on computer cluster**, and Spark
**support multiple widely used programming language** (Python, Java,
Scala, and R), and Spark **runs anywhere** from a laptop to a cluster of
thousand of servers. This makes it an easy system to start with and
scale-up to big data processing or incredibly large scale.

-  **A Unified Computing Engine**

   -  [Unified]

      -  Spark's key driving goal is to offer a unified platform for
         writing big data applications. Spark is designed to support a
         wide range of data analytics tasks, range from simple data
         loading and SQL queries to machine learning and streaming
         computation, over the same computing engine and with a
         consistent set of APIs.

   -  [Computing Engine]

      -  Spark handles loading data from storage system and performing
         computation on it, not permanent storage as the end itself, you
         can use Spark with a wide variety of persistent storage
         systems.

         -  cloud storage system

            -  Azure Stroage

            -  Amazon S3

         -  distributed file systems

            -  Apache Hadoop

         -  key-value stroes

            -  Apache Cassandra

         -  message buses

            -  Apache Kafka

-  **A set of libraries for parallel data processing on computer
   cluster**

   -  Standard Libraries

      -  SQL and sturctured data

         -  SparkSQL

      -  machine learning

         -  MLlib

      -  stream processing

         -  Spark Streaming

         -  Structured Streaming

      -  graph analytics

         -  GraphX

   -  `External Libraries <https://spark-packages.org/>`__ published as
      third-party packages by open source communities

.. _header-n73:

2.Spark å¼€å‘ç¯å¢ƒ
~~~~~~~~~~~~~~~~~~~~~~~~~

-  Language API

   -  Python

   -  Java

   -  Scala

   -  R

   -  SQL

-  Dev Env

   -  local

      -  `Java(JVM) <https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html>`__

      -  `Scala <https://www.scala-lang.org/download/>`__

      -  `Python interpreter(version 2.7 or
         later) <https://repo.continuum.io/archive/>`__

      -  `R <https://www.r-project.org/>`__

      -  `Spark <https://spark.apache.org/downloads.html>`__

   -  web-based version in `Databricks Community
      Edition <https://community.cloud.databricks.com/>`__

.. _header-n107:

3.Spark's Interactive Consoles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Python:

.. code:: shell

   ./bin/pyspark

Scala:

.. code:: shell

   ./bin/spark-shell

SQL:

.. code:: shell

   ./bin/spark-sql

.. _header-n114:

4.äº‘å¹³å°ã€æ•°æ®
~~~~~~~~~~~~~~~~~~~~~~~~~

-  `Project's
   Github <https://github.com/databricks/Spark-The-Definitive-Guide>`__

-  `Databricks <https://community.cloud.databricks.com/>`__

.. _header-n121:






Spark
----------

.. _header-n122:

1.Spark's Architecture
~~~~~~~~~~~~~~~~~~~~~~~~~

.. _header-n123:

**Cluster**

   Challenging: data processing

-  **Cluser(é›†ç¾¤)**:

   -  Single machine do not have enough power and resources to perform
      computations on huge amounts of information, or the user probably
      dose not have the time to wait for the computationto finish;

   -  A cluster, or group, of computers, pools the resources of many
      machines together, giving us the ability to use all the cumulative
      resources as if they were a single computer.

   -  A group of machines alone is not powerful, you need a framework to
      coordinate work across them. Spark dose just that, managing and
      coordinating the execution of task on data across a cluster of
      computers.

-  **Cluster manager(é›†ç¾¤ç®¡ç†å™¨)**:

   -  Spark's standalone cluster manager

   -  YARN

   -  Mesos

.. _header-n145:

**Spark Application**

-  **Cluster Manager**

   -  A **Driver** process

      -  the heart of a Spark Appliction and maintains all relevant
         information during the lifetime of the application;

      -  runs ``main()`` functions;

      -  sits on a node in the cluster;

      -  responsible for:

         -  maintaining information about the Spark Application

         -  responding to user's program or input

         -  analyzing, distributing and scheduling work across the
            **executors**

   -  A Set of **Executor** process

      -  responsible for actually carrying out the work that the
         **driver** assigns them

      -  repsonsible for :

         -  executing code assigned to it by the driver

         -  reporting the state of the computation on that executor back
            to the dirver node

-  **Spark Application**

   -  Spark employs a **cluster manager** that keeps track of the
      **resources** available;

   -  The **dirver** process is responsible for executing the **dirver
      program's commands** across the **executors** to complete a given
      task;

      -  The executors will be running Spark code

.. _header-n193:

2.Spark's Language API
~~~~~~~~~~~~~~~~~~~~~~~~~

-  Scala

   -  Spark's "default" language.

-  Java

-  Python

   -  ``pyspark``

-  SQL

   -  Spark support a subset of the ANSI SQL 2003 standard.

-  R

   -  Spark core

      -  ``SparkR``

   -  R community-driven package

      -  ``sparklyr``

.. _header-n225:

3.Spark's API
~~~~~~~~~~~~~~~~~~~~~~~~~

**Spark has two fundamental sets of APIS:**

-  Low-level "unstructured" APIs

   -  RDD

   -  Streaming

-  Higher-level structured APIs

   -  Dataset

   -  DataFrame

      -  ``org.apache.spark.sql.functions``

      -  Partitions

      -  DataFrame(Dataset) Methods

         -  DataFrameStatFunctions

         -  DataFrameNaFunctions

      -  Column Methods

         -  alias

         -  contains

   -  Spark SQL

   -  Structured Streaming

.. _header-n265:

4.å¼€å§‹ Spark
~~~~~~~~~~~~~~~~~~~~~~~~~

-  å¯åŠ¨ Spark's local modeã€

   -  äº¤äº’æ¨¡å¼

      -  ``./bin/spark-shell``

      -  ``./bin/pyspark``

   -  æäº¤é¢„ç¼–è¯‘çš„ Spark Application

      -  ``./bin/spark-submit``

-  åˆ›å»º ``SparkSession``

   -  äº¤äº’æ¨¡å¼ï¼Œå·²åˆ›å»º

      -  ``spark``

   -  ç‹¬ç«‹çš„ APP

      -  Scala:

         -  ``val spark = SparkSession.builder().master().appName().config().getOrCreate()``

      -  Python:

         -  ``spark = SparkSession.builder().master().appName().config().getOrCreate()``

.. _header-n304:

4.1 SparkSession
^^^^^^^^^^^^^^^^^^^^^^^^

   -  **Spark Application** controled by a **Driver** process called the
      **SparkSession**\ ï¼›

   -  **SparkSession** instance is the way Spark executes user-defined
      manipulations across the cluster, and there is a one-to-one
      correspondence between a **SparkSession** and a **Spark
      Application**;

ç¤ºä¾‹ï¼š

Scala äº¤äº’æ¨¡å¼ï¼š

.. code:: shell

   # in shell
   $ spark-shell

.. code:: scala

   // in Scala
   val myRange = spark.range(1000).toDF("number")

Scala APP æ¨¡å¼ï¼š

.. code:: scala

   // in Scala
   import org.apache.spark.SparkSession
   val spark = SparkSession 
   	.builder()
   	.master()
   	.appName()
   	.config()
   	.getOrCreate()

Python äº¤äº’æ¨¡å¼ï¼š

.. code:: shell

   # in shell
   $ pyspark

.. code:: python

   # in Pyton
   myRange = spark.range(1000).toDF("number")

Python APP æ¨¡å¼ï¼š

.. code:: python

   # in Python
   from pyspark import SparkSession
   spark = SparkSession \
   	.builder() \
   	.master() \
   	.appName() \
   	.config() \
   	.getOrCreate()

.. _header-n325:

4.2 DataFrames
^^^^^^^^^^^^^^^^^^^^^^^^

   -  A DataFrame is the most common Structured API;

   -  A DataFrame represents a table of data with rows and columns;

   -  The list of DataFrame defines the columns, the types within those
      columns is called the schema;

   -  Spark DataFrame can span thousands of computers:

   -  the data is too large to fit on one machine

   -  the data would simply take too long to perform that computation on
      one machine

.. _header-n344:

4.3 Partitions
^^^^^^^^^^^^^^^^^^^^^^^^

.. _header-n347:

4.4 Transformation
^^^^^^^^^^^^^^^^^^^^^^^^

.. _header-n348:

4.4.1 Lazy Evaluation
````````````````````````

.. _header-n349:

4.5 Action
^^^^^^^^^^^^^^^^^^^^^^^^

è½¬æ¢æ“ä½œèƒ½å¤Ÿå»ºç«‹é€»è¾‘è½¬æ¢è®¡åˆ’ï¼Œä¸ºäº†è§¦å‘è®¡ç®—ï¼Œéœ€è¦è¿è¡Œä¸€ä¸ªåŠ¨ä½œæ“ä½œ(action)ã€‚ä¸€ä¸ªåŠ¨ä½œæŒ‡ç¤º Spark åœ¨ä¸€ç³»åˆ—è½¬æ¢æ“ä½œåè®¡ç®—ä¸€ä¸ªç»“æœã€‚





.. _header-n350:

4.6 Spark UI
^^^^^^^^^^^^^^^^^^^^^^^^


-  **Spark job** represents **a set of transformations** triggered by **an individual action**, and can monitor the Spark job from the Spark UI;
-  User can monitor the progress of a Spark job through the **Spark web UI**:
-  Spark UI is available on port ``4040`` of the **dirver node**;

   -  Local Mode: ``http://localhost:4040``

-  Spark UI displays information on the state of:

   -  Spark jobs

   -  Spark environment

   -  cluster state

   -  tunning

   -  debugging



4.7 ä¸€ä¸ª ğŸŒ°
^^^^^^^^^^^^^^^^^^^^^^^^

(1) æŸ¥çœ‹æ•°æ®é›†

.. code-block:: shell

   $ head /data/flight-data/csv/2015-summary.csv

(2) è¯»å–æ•°æ®é›†

.. code-block:: scala

   // in Scala
   val flightData2015 = spark
      .read
      .option("inferSchema", "true")
      .option("header", "true")
      .csv("/data/flight-data/csv/2015-summary.csv")

.. code-block:: python

   # in Python
   flightData2015 = spark \
      .read \
      .option("inferSchema", "true") \
      .option("header", "true") \
      .csv("/data/flight-data/csv/2015-summary.csv")

(3) åœ¨æ•°æ®ä¸Šæ‰§è¡Œè½¬æ¢æ“ä½œå¹¶æŸ¥çœ‹ Spark æ‰§è¡Œè®¡åˆ’

.. code-block:: scala
   
   // in Scala
   // è½¬æ¢æ“ä½œ .sort()
   flightData2015.sort("count").explain()
   flightData2015.sort("count")


(4) åœ¨æ•°æ®ä¸ŠæŒ‡å®šåŠ¨ä½œæ“ä½œæ‰§è¡ŒæŠ€æœ¯

.. code-block:: scala

   // in Scala
   // é…ç½® Spark shuffle
   spark.conf.set("spark.sql.shuffle.partitions", "5")
   // åŠ¨ä½œæ“ä½œ .take(n)
   flightData2015.sort("count").take(2)


(5) DataFrame å’Œ SQL

.. code-block:: scala

   // in Scala
   flightData2015.createOrReplaceTempView("flight_data_2015")


.. code-block:: scala
   
   // in Scala
   val sqlWay = spark.sql("""
      SELECT DEST_COUNTRY_NAME, count(1)
      FROM flight_data_2015
      GROUP BY DEST_COUNTRY_NAME
      """)

   val dataFrameWay = flightData2015
      .groupBy("DEST_COUNTRY_NAME")
      .count()
   
   sqlWay.explain()
   dataFrameWay.explain()


.. code-block:: python

   # in Python
   sqlWay = spark.sql("""
      SELECT DEST_COUNTRY_NAME, count(1)
      FROM flight_data_2015
      GROUP BY DEST_COUNTRY_NAME
      """)
   
   dataFrameWay = flightData2015 \
      .groupBy("DEST_COUNTRY_NAME") \
      .count()

   sqlWay.explain()
   dataFrameWay.explain()



.. code-block:: scala

   // in Scala
   spark.sql("""
      SELECT max(count) 
      FROM flight_data_2015
      """)
      .take(1)
   
   import org.apache.spark.sql.functions.max
   flightData2015
      .select(max("count"))
      .take(1)


.. code-block:: python

   // in Python
   spark.sql("""
      SELECT max(count)
      FROM flight_data_2015
      """) \
      .take(1)

   from pyspark.sql.functions import max
   flightData2015.select(max("count")).take(1)




Spark å·¥å…·
----------------


1.Spark åº”ç”¨ç¨‹åº
~~~~~~~~~~~~~~~~~~~~~~

Spark å¯ä»¥é€šè¿‡å†…ç½®çš„å‘½ä»¤è¡Œå·¥å…· ``spark-submit`` è½»æ¾åœ°å°†æµ‹è¯•çº§åˆ«çš„äº¤äº’ç¨‹åºè½¬åŒ–ä¸ºç”Ÿäº§çº§åˆ«çš„åº”ç”¨ç¨‹åº.

é€šè¿‡ä¿®æ”¹ ``spark-submit`` çš„ ``master`` å‚æ•°ï¼Œå¯ä»¥å°†å°†åº”ç”¨ç¨‹åºä»£ç å‘é€åˆ°ä¸€ä¸ªé›†ç¾¤å¹¶åœ¨é‚£é‡Œæ‰§è¡Œï¼Œåº”ç”¨ç¨‹åºå°†ä¸€ç›´è¿è¡Œï¼Œç›´åˆ°æ­£ç¡®é€€å‡ºæˆ–é‡åˆ°é”™è¯¯ã€‚åº”ç”¨ç¨‹åºéœ€è¦åœ¨é›†ç¾¤ç®¡ç†å™¨çš„æ”¯æŒä¸‹è¿›è¡Œï¼Œå¸¸è§çš„é›†ç¾¤ç®¡ç†å™¨æœ‰ Standaloneï¼ŒMesos å’Œ YARN ç­‰.

ç¤ºä¾‹ 1ï¼š
^^^^^^^

.. code-block:: shell

   ./bin/spark-submit \
      --class org.apache.spark.examples.SparkPi \      # è¿è¡Œçš„ç±» 
      --master local \                                 # åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œç¨‹åº
      ./examples/jars/spark-examples_2.11-2.2.0.jar 10 # è¿è¡Œçš„ JAR åŒ…

ç¤ºä¾‹ 2ï¼š
^^^^^^^^^

.. code-block:: shell
   
   ./bin/spark-submit \
      -- master local \
      ./examples/src/main/python/pi.py 10


2.Dataset: ç±»å‹å®‰å…¨çš„ç»“æœåŒ– API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



ç¤ºä¾‹:
^^^^^^^^^

.. code-block:: scala

   case class Flight(DEST_COUNTRY_NAME: String, 
                     ORIGIN_COUNTRY_NAME: String,
                     count: BigInt)
   val flightDF = spark
      .read
      .parquet("/data/flight-data/parquet/2010-summary.parquet/")
   
   val flights = flightDF.as[Flight]

   flights
      .fliter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
      .map(flight_row => flight_row)
      .take(5)
   
   flights
      .take(5)
      .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
      .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))



3.Spark Structured Streaming
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Spark Structured Streaming(Spark ç»“æ„åŒ–æµå¤„ç†) æ˜¯ç”¨äºæ•°æ®æµå¤„ç†çš„é«˜é˜¶ APIï¼Œ
åœ¨ Spark 2.2 ç‰ˆæœ¬ä¹‹åå¯ç”¨ã€‚å¯ä»¥åƒä½¿ç”¨ Spark ç»“æ„åŒ– API åœ¨æ‰¹å¤„ç†æ¨¡å¼ä¸‹ä¸€æ ·ï¼Œ
æ‰§è¡Œç»“æ„åŒ–æµå¤„ç†ï¼Œå¹¶ä»¥æµå¼æ–¹å¼è¿è¡Œå®ƒä»¬ï¼Œä½¿ç”¨ç»“æ„åŒ–æµå¤„ç†å¯ä»¥å‡å°‘å»¶è¿Ÿå¹¶å…è®¸å¢é‡å¤„ç†.
æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒå¯ä»¥å¿«é€Ÿåœ°ä»æµå¼ç³»ç»Ÿä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè€Œä¸”å‡ ä¹ä¸éœ€è¦æ›´æ”¹ä»£ç ã€‚
å¯ä»¥æŒ‰ç…§ä¼ ç»Ÿæ‰¹å¤„ç†ä½œä¸šçš„æ¨¡å¼è¿›è¡Œè®¾è®¡ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºæµå¼ä½œä¸šï¼Œå³å¢é‡å¤„ç†æ•°æ®ï¼Œ
è¿™æ ·å°±ä½¿å¾—æµå¤„ç†å˜å¾—å¼‚å¸¸ç®€å•.


æ•°æ®é›†ï¼šhttps://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data

1.åˆ›å»ºä¸€ä¸ªé™æ€æ•°æ®é›† DataFrame ä»¥åŠ Schema
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: scala

   // in Scala
   val staticDataFrame = spark
      .read
      .format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("/data/retail-data/by-day/*.csv")
   
   staticDataFrame.createOrReplaceTempView("retail_data")
   cal staticSchema = staticDataFrame.schema

.. code-block:: python

   # in Python
   staticDataFrame = spark \
      .read \
      .format("csv") \
      .option("header", "true") \
      .option("inferSchema", "true") \
      .load("/data/retail-data/by-day/*.csv")
   
   staticDataFrame.createOrReplaceTempView("retail_data")
   staticSchema = staticDataFrame.schema


2.å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆæ“ä½œ
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: scala

   // in Scala
   import org.apache.spark.sql.functions.{window, column, desc, col}
   staticDataFrame
      .selectExpr(
         "CustomerId", 
         "(UnitPrice * Quantity) as total_cost", 
         "InvoiceDate"
      )
      .groupBy(
         col("CustomerId"), 
         window(col("InvoiceDate"), "1 day")
      )
      .sum("total_cost")
      .show(5)


.. code-block:: python

   # in Python
   from pyspark.sql.functions import window, column, desc, col
   staticDataFrame \
      .selectExpr(
         "CustomerId", 
         "(UnitPrice * Quantity) as total_cost", 
         "InvoiceDate"
      ) \
      .groupBy(
         col("CustomerId"), 
         window(col("InvoiceDate"), "1 day")
      ) \
      .sum("total_cost") \
      .show(5)


3.è®¾ç½®æœ¬åœ°æ¨¡å‹è¿è¡Œå‚æ•°é…ç½®
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: scala

   // in Scala
   spark.conf.set("spark.sql.shuffle.partitions", "5")


.. code-block:: python

   # in Python
   spark.conf.set("spark.sql.shuffle.partitions", "5")



4.å°†æ‰¹å¤„ç†ä»£ç è½¬æ¢ä¸ºæµå¤„ç†ä»£ç 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(1)è¯»å–æµå¼æ•°æ®ï¼š

.. code-block:: scala

   // in Scala
   val streamingDataFrame = spark
      .readStream
      .schema(staticSchema)
      .option("maxFilesPerTrigger", 1)       // æŒ‡å®šä¸€æ¬¡åº”è¯¥è¯»å…¥çš„æ–‡ä»¶æ•°é‡ï¼Œåœ¨å®é™…åœºæ™¯ä¸­è¢«çœç•¥
      .format("csv")
      .option("header", "true")
      .load("/data/retail-data/by-day/*.csv")


.. code-block:: python

   # in Python
   streamingDataFrame = spark \
      .readStream \
      .schema(staticSchema) \
      .option("maxFilesPerTrigger", 1) \
      .format("csv") \
      .option("header", "true") \
      .load("/data/retail-data/by-day/*.csv")

(2)æŸ¥çœ‹ DataFrame æ˜¯å¦ä»£è¡¨æµæ•°æ®ï¼š

.. code-block:: scala

   // in Scala
   streamingDataFrame.isStreaming // è¿”å› true


.. code-block:: python

   # in Python
   streamingDataFrame.isStreaming # è¿”å› true


(3)å¯¹æµå¼æ•°æ®æ‰§è¡Œåˆ†ç»„èšåˆæ“ä½œ(è½¬æ¢æ“ä½œ)

.. code-block:: scala

   # in Scala
   val purchaseByCustomerPerHour = streamingDataFrame
      .selectExpr(
         "CustomerId", 
         "(UnitPrice * Quantity) as total_cost", 
         "InvoiceDate"
      )
      .groupBy(
         $"CustomerId", 
         window($"InvoiceDate", "1 day")
      )
      .sum("total_cost")


.. code-block:: python

   # in Python
   purchaseByCustomerPerHour = streamingDataFrame \
      .selectExpr(
         "CustomerId", 
         "(UnitPrice * Quantity) as total_cost", 
         "InvoiceDate"
      ) \
      .groupBy(
         col("CustomerId"), 
         window(col("InvoiceDate"), "1 day")
      ) \
      .sum("total_cost") \
      .show(5)


(4)è°ƒç”¨å¯¹æµæ•°æ®çš„åŠ¨ä½œæ“ä½œï¼Œå°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜ä¸­çš„ä¸€ä¸ªè¡¨ä¸­ï¼Œåœ¨æ¯æ¬¡è¢«è§¦å‘åæ›´æ–°è¿™ä¸ªå†…å­˜ç¼“å­˜

.. code-block:: scala

   // in Scala
   purchaseByCustomerPerHour.writeStream
      .format("memory")               // memory ä»£è¡¨å°†è¡¨å­˜å…¥å†…å­˜
      .queryName("customer_purchases") // å­˜å…¥å†…å­˜çš„è¡¨çš„åç§°
      .outputMode("complete")         // complete è¡¨ç¤ºä¿å­˜è¡¨ä¸­æ‰€æœ‰è®°å½•
      .start()

.. code-block:: python

   # in Python
   purchaseByCustomerPerHour.writeStream \
      .format("memory") \
      .queryName("customer_purchases") \
      .outputMode("complete") \
      .start()

(5)è¿è¡ŒæŸ¥è¯¢è°ƒè¯•ç»“æœ

.. code-block:: scala

   // in Scala
   spark.sql("""
      SELECT * 
      FROM customer_purchases
      ORDER BY `sum(total_cost)` DESC
      """)
      .show(5)


.. code-block:: python

   # in Python
   spark.sql("""
      SELECT * 
      FROM customer_purchases
      ORDER BY `sum(total_cost)` DESC
      """) \
      .show(5)

(6)å°†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°

.. code-block:: scala

   // in Scala
   purchaseByCustomerPerHour.writeStream
      .format("console")
      .queryName("customer_purchases_2")
      .outputMode("complete")
      .start()


.. code-block:: python

   # in Python
   purchaseByCustomerPerHour.writeStream \
      .format("console") \
      .queryName("customer_purchases_2") \
      .outputMode("complete") \
      .start()


4.Spark æœºå™¨å­¦ä¹ å’Œé«˜çº§æ•°æ®åˆ†æ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


5.Spark ä½é˜¶ API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Spark ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æ˜¯å»ºç«‹åœ¨ RDD ä¹‹ä¸Šçš„. Spark çš„é«˜é˜¶ API åŠæ‰€æ”¯æŒçš„é«˜çº§æ“ä½œéƒ½ä¼šè¢«ç¼–è¯‘åˆ°è¾ƒä½çº§çš„ RDD ä¸Šæ‰§è¡Œï¼Œä»¥æ–¹ä¾¿å’Œå®ç°å…¶è¾ƒé«˜æ•ˆçš„åˆ†å¸ƒå¼æ‰§è¡Œ. ä½¿ç”¨ RDD å¯ä»¥å¹¶è¡ŒåŒ–å·²ç»å­˜å‚¨åœ¨é©±åŠ¨å™¨æœºå™¨å†…å­˜ä¸­çš„åŸå§‹æ•°æ®.

å¤§å¤šæ•°æƒ…å†µä¸‹ç”¨æˆ·åªéœ€è¦ä½¿ç”¨ Spark çš„é«˜é˜¶ API æˆ–é«˜çº§æ“ä½œå°±å¯ä»¥å®ç°æ‰€éœ€çš„ä¸šåŠ¡é€»è¾‘ï¼Œæœ‰æ—¶å€™å¯èƒ½éœ€è¦ä½¿ç”¨ RDDï¼Œç‰¹åˆ«æ˜¯åœ¨è¯»å–æˆ–æ“ä½œåŸå§‹æ•°æ®(æœªå¤„ç†æˆ–éç»“æ„åŒ–çš„æ•°æ®)æ—¶.


ç¤ºä¾‹ 1:
^^^^^^^^^^^

.. code-block:: scala

   // in Scala
   spark.sparkContext.parallelize(Seq(1, 2, 3)).toDF() // å°† RDD è½¬åŒ–ä¸º DataFrame

ç¤ºä¾‹ 2:
^^^^^^^^^^^

.. code-block:: python

   # in Python
   from pyspark.sql import Row

   spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()



6.SparkR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SparkR æ˜¯ä¸€ä¸ªåœ¨ Spark ä¸Šè¿è¡Œçš„ R è¯­è¨€å·¥å…·ï¼Œå®ƒå…·æœ‰ä¸ Spark å…¶ä»–æ”¯æŒè¯­è¨€ç›¸åŒçš„è®¾è®¡å‡†åˆ™. SparkR ä¸ Spark çš„ Python API éå¸¸ç›¸ä¼¼ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒSparkR æ”¯æŒ Python æ”¯æŒçš„æ‰€æœ‰åŠŸèƒ½

ç¤ºä¾‹ 1:
^^^^^^^^^
.. code-block:: r

   # in R
   library(SparkR)
   sparkDf <- read.df("/data/flight-data/csv/2015-summary.csv", source = "csv", header = "true", inferSchema = "true")
   take(sparkDF, 5)
   collect(orderBy(sparkDF, "count"), 20)

ç¤ºä¾‹ 2:
^^^^^^^^^

.. code-block:: r

   # in R
   library(magrittr)

   sparkDF %>% 
      orderBy(desc(sparkDF$count)) %>%
      groupBy("ORIGIN_COUNTRY_NAME") %>%
      count() %>%
      limit(10) %>%
      collect()




7.Spark ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·åŒ…
~~~~~~~~~~~~~~~~~~~~~~~~~~

å¯ä»¥åœ¨ `Spark Packages ç´¢å¼•
<https://spark-packages.org>`_ æ‰¾åˆ°æ‰€æœ‰çš„å¼€æºç¤¾åŒºç»´æŠ¤çš„å·¥å…·åŒ…ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥å°†è‡ªå·±å¼€å‘çš„å·¥å…·åŒ…å‘å¸ƒåˆ°æ­¤ä»£ç åº“ä¸­ï¼Œä¹Ÿå¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°å„ç§å…¶ä»–é¡¹ç›®å’Œå·¥å…·åŒ….